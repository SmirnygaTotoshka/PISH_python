# Анализ данных. SciPy, Seaborn

## Введение

Python - самый популярный язык в анализе данных благодаря простоте самого языка и специализированным пакетам. SciPy - пакет для научных вычислений (SciPy - scientific python). Основан на библиотеке numpy и содержит модули для анализа сигналов, линейной алгебры и численных методов математического анализа, анализа изображений, работы с пространственными данными и статистике. Seaborn - библиотека для визуализации, являющаяся обощением и развитием matplotlib и основана на ней. Она содержит функции для построения сложных графиков по таблицам pandas. То, что в matplotlib может занять 10 строчек кода, seaborn сделает за одну. Библиотека по умолчанию строит графики в приятной пастельной палитре морских цветов, за что и получила своё название. Эти библиотеки также имеют свои собственные официальные сайты, продвинутую документацию и обучающие материалы ([scipy](https://scipy.org/), [seaborn](https://seaborn.pydata.org/)).

Аналитики данных работают в блокнотах Jupyter. Jupyter Notebook - среда разработки, позволяющая выполнять код на основных аналитических языках програмированния (Python, R, Julia, Scala и многие другие) пошагово в ячейках. Таким образом составлять красивые отчеты, которые могут быть интерактивными. Блокноты имеют расшинение .ipynb. Данное пособие также сверстано с помощью Jupyter Notebook. Экосистема Jupyter содержит также программу Jupyter Lab, более продвинутую среду разработки блокнотов, так как дает функционал управления компьютером через командную строку, работу с системой контроля версий и управление плагинами и расширениями, которыми можно делать работу в Jupyter Lab комфортнее и производительней.

Настроим себе окружение для работы: создадим вирутальное окружение conda с именем stat. Пакеты будем брать из канала conda-forge. Для работы нам потребуется:

1.  pandas - для работы с таблицами
2.  scipy - для статистических тестов
3.  scikit-learn - библиотека для машинного обучения. Из неё мы возьмем только набор данных для демонстрации.
4.  ipykernel, jupyterlab - Программа Jupyter Lab для создания блокнотов, кроме программы нужна специальная библиотека ядро, которое будет запускать код в блокноте.
5.  seaborn - для визуализации

Указывать numpy и matplotlib не нужно. Они идут как зависимости к перечисленным пакетам и conda позаботится об установки нужной версии этих пакетов.

``` bash
conda create -n stat -c conda-forge pandas scipy sсikit-learn ipykernel seaborn jupyterlab
conda activate stat
```

## Разведывательный анализ

В качестве примера мы рассмотрим популярный в обучении машинному обучению набор данных - [ирисы Фишера](https://ru.wikipedia.org/wiki/%D0%98%D1%80%D0%B8%D1%81%D1%8B_%D0%A4%D0%B8%D1%88%D0%B5%D1%80%D0%B0). Это таблица с данными о 150 цветках 3 видов ирисов: щетинистый (*Iris setosa*), виргинский (*Iris virginica*), разноцветный (*Iris versicolor*). Для каждого цветка измерялись:

1.  Длина наружной доли околоцветника (англ. sepal length);
2.  Ширина наружной доли околоцветника (англ. sepal width);
3.  Длина внутренней доли околоцветника (англ. petal length);
4.  Ширина внутренней доли околоцветника (англ. petal width).

Импортируем библиотеки и сделаем небольшой маневр по преобразованию данных ириса из внутреннего представления библиотеки scikit-learn в привычный датафрейм.

```{python}
import pandas as pd
import matplotlib.pyplot as plt
from scipy import stats
import seaborn as sns
from sklearn import datasets

iris = datasets.load_iris() # <1>
iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names) # <2>
iris_df['species'] = iris.target_names[iris.target] # <3>
iris_df.head() # <4>
```

1.  Загружаем датасет ирисов из модуля библиотеки scikit-learn
2.  Преобразуем датасет в pandas.DataFrame. Пока он не содержит названия видов для записей
3.  Добавляем название видов
4.  Смотрим на корректность создания таблицы

Первый этап анализа данных - убедиться, что всё считалось правильно. Для этого как правило смотрят на первые и последние n-строк. Затем необходимо посмотреть на типы данных в столбцах. Мы имеем 4 столбца с числовыми характеристиками, нет пропущенных значений. 1 столбец с строками для обозначения вида цветка.

```{python}
iris_df.info()
```

Следующий этап - описательная статистика. Для числовых характеристик выполняется вызовом одной функции describe.

```{python}
iris_df.describe()
```

Для качественных характеристик можно посчитать количество записей для каждого класса. Наш датасет идеально сбалансирован. В каждом классе одинаковое количество записей.

```{python}
iris_df["species"].value_counts()
```

Следующий этап - визуализация. Посмотрим на распределения параметров. Чаще всего это делают с помощью гистограмм и "ящиков с усами". Воспользуемся библиотекой seaborn для быстрого построения графиков.

```{python}
fig, ax = plt.subplots(figsize = (8,6)) # <1>
sns.boxplot(data = iris_df, x = "species", y = "sepal length (cm)",hue = "species", ax = ax) # <2>
fig.savefig("images/example1.png", dpi = 300) # <3>
```

1.  Создаем рисонок и оси. Задаем размер рисунка 8\*6 дюймов.
2.  Строим seaborn.boxplot. sns - общепринятый псевдоним. В качестве данных передаем наш датафрейм и указываем имена столбцов, которые необходимо использовать для рисования: x - виды, y - значение параметра. Раскрасить в зависимости от вида. Чтобы seaborn рисовал в созданных нами осях, их нужно передать в качетсве параметра.
3.  Сохраняем изображение в формате png с 300 точками на дюйм.

Можно не передавать оси в качестве параметра. Тогда seaborn создаст свой рисунок и на нём изобразит график.

```{python}
sns.boxplot(data = iris_df, x = "species", y = "petal length (cm)",hue = "species")
```

Гистограммы рисовать также легко и приятно. Пример для одного вида. Параметр bins обозначает количество столбиков, на который будет разбит изображаемый интервал.

```{python}
setosa = iris_df.loc[iris_df["species"] == "setosa"]
sns.histplot(data = setosa, x = "sepal length (cm)", bins = 10)
```

Случай изображения всех классов на одном графике ещё проще. Кроме того, seaborn позаботиться о легенде.

```{python}
sns.histplot(data = iris_df, x = "sepal length (cm)", hue = "species",bins = 20)
```

## Статистические критерии

Изначально этот датасет создавался для доказательства возможности классификации цветка к определенному виду без экспертного заключения, опираясь только на количественные параметры цветка. Мы с вами тоже попробуем статистически различить виды по параметрам цветков. Для начала необходимо определить вид распределения в каждой группе (виде растения): подчиняется ли он нормальному закону или нет. Исходя из результатов проверки, мы будем применять те или иные статистические критерии.

::: callout-note
## Общая схема работы статистического критерия

1.  Утверждение нулевой гипотезы $H_0$ , которая говорит об отсутствии эффекта или различий.
2.  Утверждение альтернативных гипотез $H_i$
3.  Утверждление уровня значимости. Тот порог, по которому мы считаем, что изменения статистически значимы. Обычно выбирают 0.05. Ещё эту величину называют уровнем ошибки первого рода или вероятностью ложного срабатывания.
4.  Расчет значения статистики - некоторой функции от данных. Изначально мы предполагаем, что $H_0$ истинна, поэтому статистика должна иметь какое-то известное нам распределение.
5.  Расчет вероятности встретить такое же или большее значение статистики в случае справедливости $H_0$
6.  Если полученная вероятность меньше заданного уровня значимости, то мы "отвергаем нулевую гипотезу", иначе мы "не имеем оснований отвергнуть нулевую гипотезу".

Статистические критерии работают от противного, так как обычно только для случая отсутствия эффекта можно подобрать вычисляемое распределение статистики.
:::

С текущим развитием вычислительной техники нам ничего не мешает посчитать любой статистический критерий для любого набора данных. Но тогда встает вопрос о мощности критерия (1 - верояность ошибки второго рода, пропуска цели) и корректности сделанных выводов. Каждый критерий имеет свои допущения, в пределах которых он может корректно работать. Например, для самого известного теста на проверку разницы средних в двух группах, t-теста Стьюдента существуют следующие допущения.

1.  Данные распределены непрерывно (количественные характеристики)
2.  Данные независимы и случайны (зависит от постановки эксперемента)
3.  Нормальное распределение в группах
4.  Гомогенность (равенство) дисперсий в группах

Для проверки на соблюдения нормального закона распределения существует тест Шапиро-Уилка. Применить его очень просто.

```{python}
res = stats.shapiro(setosa["sepal length (cm)"])
res
```

Объект с результатами теста содержит значение статистики и p-value, по которому мы определяем значимость результата.

::: callout-important
## Задание

1.  Какое распределение имеет проверенный параметр: нормальное или отличное от нормального?
2.  Сколько раз необходимо применить тест Шапиро-Уилка?
3.  Напишите код, который его применяет у нужным группам и параметрам и результат сохраняет в словарь
:::

```{python}
#| code-fold: true 
#| code-summary: "Ответ на третий пункт"

species = iris_df["species"].unique()
parameters = iris_df.columns[0:-1]
pvalue = {}
for s in species:
    for p in parameters:
        df = iris_df.loc[iris_df["species"] == s, p]
        res = stats.shapiro(df)
        pvalue[f"{s}_{p}"] = res.pvalue
print(pvalue)
```

Уровень значимости 0.05 подразумевает 1 ложное срабатывание на 20 попыток применения теста, поэтому мы имеем далеко непризрачный шанс получить ошибку. Данная проблема называется проблемой множественных сравнений и её решают корректированием массива p-value. Одна из самых частых применяемых поправок - поправка Бенджамини-Хохберга, более известная как поправка на частоту ложных открытий (false discovery rate, fdr). Применим её к нагему массиву p-value.

```{python}
adj_res = stats.false_discovery_control(list(pvalue.values()))
print(adj_res.round(4))
```

Тесты на проверку равенства дисперсий: F-тест Фишера и тест Бартлетта оставляю на самостоятельное изучение. Попробуем применить t-тест Стьюдента для произвольного параметра и двух видов.

```{python}
setosa_sepal_length = iris_df.loc[iris_df["species"] == "setosa", "sepal length (cm)"]
versicolor_sepal_length = iris_df.loc[iris_df["species"] == "versicolor", "sepal length (cm)"]
stats.ttest_ind(setosa_sepal_length, versicolor_sepal_length)
```

Проинтерпретируйте результат. Правомочны ли мы применять этот критерий для этого параметра и этих групп?

## Подведение итогов

1.  Jupyter Lab - мощный инструмент аналитики, позволяющий создавать красивый отчеты в виде интерактивных блокнотов.
2.  Python располагает целой экосистемой для научных вычислений: pandas - работа с таблицами, numpy, scipy - сложные математические вычисления, matplotlib, seaborn - красивая визуализация.

## Дополнительная информация

Тема о сравнении среднего в двух группах на самом деле гораздо более дискуссионная и не сводится к простым алгоритмам, как это представлено в прикладных обучающих пособиях по статистике. Классическая теория вероятности говорит нам о том, что допущение о нормальном законе распределения величины в группах не так важно, так как при количестве наблюдений n \< 30, мы можем установить характер распределения с большим уровнем ошибки и априорных допущений, а при n \> 30 распределение Стьюдента хорошо аппроксимируется нормальным распределением вследствие Центральной Предельной Теоремы. Несмотря на то, что критерии Манн-Уитни и Вилкоксона преподносятся как непараметрические аналоги t-теста Стьюдента, они проверяет несколько иные нулевые гипотезы. Автор призывает вас не верить слепо гайдам по прикладной биостатистике, и знать как работают используемые в вашей работе критерии.