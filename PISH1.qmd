# Арифметические основы работы компьютера

## Лирическое отступление

Основной рабочий инструмент биоинформатика - компьютер. Все то множество операций, которые мы можем совершать с компьютером, можно разбить на арифметические и логические операции. Для совершения таких операций компьютер должен кодировать информацию в таком виде, в котором он сможет с ней работать. Современные компьютеры используют бинарную кодировку, то есть любая информация представляется в виде последовательности 0 и 1 (битов). Минимально доступная ячейка памяти для процессора называется байтом. 1 байт равен 8 битам.

::: callout-important
## Обратите внимание

Кратные величины байта содержат в себе 1024 единицы, а не 1000. Например, 1 Кбайт равен 1024 байт.
:::

Цель текущей главы разобраться в низкоуровневых стандартах кодирования информации компьютером и выяснить, какие это имеет последствия для нас, как программистов.

## Представление целых чисел

Целое число можно легко представить в бинарном виде путем последовательного деления на два до цифры меньше основания системы счисления (т.е. до 1). и записи этой 1 с остатками в обратном порядке.

![Перевод числа из десятичной системы счисления в двоичную.](images/1_CS.png){#fig-cs_dec_to_bin}

Процесс перевода показан на @fig-cs_dec_to_bin. После деления идем с конца в начало и получаем, что $567_{10} = 1000110111_2$. Однако, существуют особенности кодирования чисел с знаком и без знака

### Без знака

Без знака число последовательно записывается в ячейку памяти от старшего (большего по номеру, позиции) разряда к меньшему. Пример записи представлен в @tbl-int_unsigned. Число $84_{10}$ было переведено в двоичный вид $1010100_2$ и записано с использованием всех доступных разрядов. Пустые разряды заполняются нулями. Хранимый диапазон чисел при таком подходе $[0; 2^k-1]$, где k - количество бит, выделенных под хранение числа.

| 7   | 6   | 5   | 4   | 3   | 2   | 1   | 0   |
|-----|-----|-----|-----|-----|-----|-----|-----|
| 0   | 1   | 0   | 1   | 0   | 1   | 0   | 0   |

: Расположение закодированного числа без знака в ячейке памяти. Верхняя строка - номер разряда, нижняя - значение в этом разряде. {#tbl-int_unsigned}

### C знаком

В случае, если нам необходимо хранить ещё и отрицательные числа, тогда необходимо выделить память под знак. Для этого выделяют старший разряд числа: 0 - положительное число, 1 - отрицательное число. В @tbl-int_signed представлена запись числа $-84_{10}$. Хранимый диапазон чисел при таком подходе $[-2^{(k-1)}; 2^{k-1}-1]$, где k - количество бит, выделенных под хранение числа.

| 7   | 6   | 5   | 4   | 3   | 2   | 1   | 0   |
|-----|-----|-----|-----|-----|-----|-----|-----|
| 1   | 1   | 0   | 1   | 0   | 1   | 0   | 0   |

: Расположение закодированного числа с знаком в ячейке памяти. Верхняя строка - номер разряда, нижняя - значение в этом разряде. {#tbl-int_signed}

::: callout-tip
## Посчитайте!

1.  Какое максимальное число без знака можно закодировать в 8 битах?
2.  Какое диапазон чисел с знаком можно закодировать в 2 байтах?
:::

Важное следствие такого формата следующее: если попробовать сохранить число, выходящие за рамки максимально допустимого при выделенном объёме памяти, то произойдет ошибка переполнения. Чтобы прочувствовать её, попробуйте записать число 256 в двоичном виде в 8 ячеек.

## Представление действительных чисел

Для начала необходимо познакомиться с тем, как можно перевести десятичную дробь в двоичный вид. Целая часть переводится по разобранному выше алгоритму. Процесс перевода дробной части показан на @fig-float_bin. Дробная часть представляется в виде дробе, где целая часть равна нулю. Затем такая дробь умножается на два (основание системы счисления). В результате получаем дробь, чью целую часть записываем как цифру дробной части, адробную часть снова представляем в виде дроби с нулевой целой частью. Процесс повторяется, пока в результате умножения не получим дробную часть равную 0.

![Процесс перевода дробных частей в разные системы счисления](images/1_float_to%20bin.png){#fig-float_bin}

На вход такого преобразования мы можем получать бесконечные дроби, так и результат преобразования конечной дроби может давать бесконечную дробь, а память конечна. Поэтому вещественные числа мы можем хранить только с заданной точностью. Точность задается количеством бит, выделенных под хранение числа. Вещественное число хранят в так называемом нормализованном в виде. Этот вид представлен в @eq-norm_number.

$$
N = M * q^p
$$ {#eq-norm_number}

N - число; M - мантисса, $M \in [0.1;1]$; q - основание системы счисления; p - порядок

Например, $752.13 = 0.75213*10^3$

Схема представления вещественных чисел представлена на @fig-ieee_754. Разберем её на примере числа с одинарной точностью (float). Старший разряд отдается на хранение знака числа, следующие восемь бит - для порядка. Причем, чтобы не выделять память на хранение знака порядка, его хранят в смещенном на константу виде. В случае одинарной точности эта константа равна 128. Таким образом, можно закодировать отрицательные порядке в формате положительного числа. Оставшиеся 23 бита отдают для хранения мантиссы, которая переводится в двоичный вид по алгоритму, представленному в этой подглаве.

![Схема стандарта хранения действительных чисел IEEE-754](images/1_ieee-754.png){#fig-ieee_754 width="508"}

Из такого формата следует, что необходимо быть максимально аккуратным с арифметическими операциями с числами, хранящимся в таком формате. Самый простой пример.

```{python}
0.1 + 0.2
```

Как можете увидеть, результат выполнения не равен 0.3! Также легко может оказаться, что при вычитании двух одинаковых чисел вы не получите 0, а какое-то очень маленькое число. Эта проблема решается округлением.

## Представление текстовой информации

С любым другим видом информации невозможен простой перевод в двоичный вид. Поэтому их переводят в численный вид, а затем числа переводят в двоичный вид. Структурная единица текста - символ. Для символов придумано большое количество таблиц соответствия символов их числовым кодам. Одна из самых первых и использующихся до сих пор - таблица ASCII, представленная на @fig-ascii. Однако наиболее удолетворяющая современным требованиям формат кодирования текста - Unicode и его вариация utf-8. Автор рекомендует в свое работе использовать этот формат кодировки.

![Таблица ASCII в своем изначальном виде.](images/1_ascii.png){#fig-ascii}

::: callout-note
## Занимательный факт

Таблица ASCII активно используется в биоинформатике для компактного кодирования качества результатов секвенирования.
:::

Бывают случаи, когда файл, сохранённой в одной кодировке пробуют открыть в другой. Тогда вместо понятных символов пользователь получает непонятные иероглифы и крякозябры.

## Представление звуковой и графической информации

Звук и графику легче представить в числовом виде. Звук есть ни что иное, как измерянные в определенные моменты времени амплитуды колебания сигнала. Т.е. в самом простом виде запись звука - таблица из двух столбцов: времени и амплитуды, которые являются числами и легко переводятся в двоичный формат. С графикой несколько сложнее. Графическая информация представляет собой сохраненные состояния пискселей - структурных единиц монитора. Эти состояния могут описываться в разных системах координат. Самая распространённая - RGB. Согласно ей состояние пискселя - есть суперпозиция из трёх основных цветов: красного (Red), зелёного (Green), синего (Blue). На каждый цвет выделяется 1 байт. Градация цвета идут от 0 (чисто чёрный) до 255 (чисто белый). Цвет принято записывать в шестнадцетиричном виде, где первые две цифры - доля красного, затем доля зелёного, затем доля синего. Например #7634AC.

::: callout-tip
## Попробуйте

Зайдите на [сайт по конвертации кодов в цвет](https://www.rapidtables.com/web/color/RGB_Color.html) и узнайте, что за цвет 7634AC.
:::

## Подведение итогов

1.  Всё, что делает компьютер - арифметические и логические операции
2.  Целые числа и вещественные числа (числа с плавающей точкой) хранятся по разному
3.  Вся прочая информация переводится в численный вид, а затем в двоичный.
4.  Принципы кодирования и хранения имеют свои особенности, которые должен учитывать программист при работе.

## Задание

1.  Переведите в двоичный вид: 134, -17, 12.5, -134.4. Для записи целых чисел используйте 8 бит, для дробных - 32 бита.
2.  По таблице ASCII @fig-ascii переведите в двоичный код сообщение (без кавычек) "Hello, world!". На один символ отводится 1 байт.